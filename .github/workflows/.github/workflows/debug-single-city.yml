name: Debug Single City Scrape

on:
  workflow_dispatch:
    inputs:
      city_index:
        description: 'City index to test (0-199)'
        required: false
        default: '0'
        type: string
      use_proxy:
        description: 'Use proxies'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  debug-scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install pandas==2.0.3 numpy==1.24.3 playwright more_itertools aiohttp
          fi
          
          # Install playwright browsers
          playwright install chromium
          playwright install-deps

      - name: Prepare test city
        run: |
          CITY_INDEX=${{ github.event.inputs.city_index }}
          
          # Get the specific city
          CITY=$(sed -n "$((CITY_INDEX + 1))p" cities_top200.txt)
          echo "Testing city: $CITY (index $CITY_INDEX)"
          echo "$CITY" > test_city.txt

      - name: Decrypt proxy file if needed
        if: ${{ github.event.inputs.use_proxy == 'true' }}
        env:
          PROXY_PASSWORD: ${{ secrets.PROXY_PASSWORD }}
        run: |
          echo "Decrypting proxy file..."
          openssl enc -d -aes-256-cbc -pbkdf2 -in formatted_proxies.txt.enc -out formatted_proxies.txt -k "$PROXY_PASSWORD"

      - name: Run detailed debug scrape
        run: |
          USE_PROXY=${{ github.event.inputs.use_proxy }}
          
          # Create debug scraper script
          cat > debug_scraper.py << 'EOF'
          import asyncio
          import sys
          from playwright.async_api import async_playwright
          from datetime import datetime, timedelta
          import logging
          
          logging.basicConfig(level=logging.DEBUG)
          
          async def test_scrape():
              city = open('test_city.txt').read().strip()
              print(f"Testing scrape for: {city}")
              
              browser = None
              try:
                  async with async_playwright() as p:
                      print("Launching browser...")
                      browser = await p.chromium.launch(
                          headless=True,
                          args=['--disable-blink-features=AutomationControlled']
                      )
                      
                      print("Creating context...")
                      context = await browser.new_context(
                          viewport={'width': 1920, 'height': 1080},
                          user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      )
                      
                      page = await context.new_page()
                      
                      # Enable detailed logging
                      page.on("console", lambda msg: print(f"Browser console: {msg.text}"))
                      page.on("pageerror", lambda err: print(f"Page error: {err}"))
                      
                      print("Navigating to AA Hotels...")
                      try:
                          await page.goto("https://www.aadvantagehotels.com/", timeout=60000)
                          print("Page loaded successfully")
                      except Exception as e:
                          print(f"Failed to load page: {e}")
                          # Take screenshot
                          await page.screenshot(path="error_screenshot.png")
                          raise
                      
                      await asyncio.sleep(2)
                      
                      print("Looking for search input...")
                      try:
                          city_input = await page.wait_for_selector(
                              'input[placeholder="Enter a city, airport, or landmark"]',
                              timeout=15000
                          )
                          print("Found search input")
                      except:
                          print("Search input not found, trying alternatives...")
                          # Try other selectors
                          selectors = [
                              'input[type="search"]',
                              'input[name="destination"]',
                              '#destination',
                              '.search-input'
                          ]
                          for sel in selectors:
                              try:
                                  city_input = await page.wait_for_selector(sel, timeout=5000)
                                  print(f"Found input with selector: {sel}")
                                  break
                              except:
                                  continue
                          else:
                              await page.screenshot(path="no_input_screenshot.png")
                              raise Exception("Could not find city search input")
                      
                      await city_input.click()
                      await asyncio.sleep(1)
                      await city_input.fill(city)
                      print(f"Filled city: {city}")
                      await asyncio.sleep(2)
                      
                      # Take screenshot after filling
                      await page.screenshot(path="after_fill_screenshot.png")
                      
                      print("Looking for dropdown options...")
                      dropdown_options = await page.query_selector_all('ul[role="listbox"] li')
                      if dropdown_options:
                          print(f"Found {len(dropdown_options)} dropdown options")
                          await dropdown_options[0].click()
                      else:
                          print("No dropdown, trying keyboard navigation")
                          await page.keyboard.press('ArrowDown')
                          await page.keyboard.press('Enter')
                      
                      # Set dates
                      checkin = datetime.now() + timedelta(days=1)
                      checkout = checkin + timedelta(days=1)
                      checkin_str = checkin.strftime("%m/%d/%Y")
                      checkout_str = checkout.strftime("%m/%d/%Y")
                      
                      print(f"Setting dates: {checkin_str} to {checkout_str}")
                      
                      await page.evaluate(f'''
                          document.querySelector('input[placeholder="Check-in"]').value = "{checkin_str}";
                          document.querySelector('input[placeholder="Check-out"]').value = "{checkout_str}";
                      ''')
                      
                      await asyncio.sleep(1)
                      
                      print("Clicking search button...")
                      await page.get_by_role("button", name="Search").click()
                      
                      print("Waiting for results...")
                      try:
                          await page.wait_for_url("**/search**", timeout=45000)
                          print("Results page loaded")
                      except:
                          print("Timeout waiting for results")
                          await page.screenshot(path="timeout_screenshot.png")
                          raise
                      
                      # Take final screenshot
                      await page.screenshot(path="results_screenshot.png")
                      
                      print("Success! Check screenshots for visual confirmation")
                      
              except Exception as e:
                  print(f"Error during scrape: {type(e).__name__}: {e}")
                  import traceback
                  traceback.print_exc()
              finally:
                  if browser:
                      await browser.close()
          
          if __name__ == "__main__":
              asyncio.run(test_scrape())
          EOF
          
          # Run the debug scraper
          python debug_scraper.py

      - name: Run actual scraper with single city
        run: |
          USE_PROXY=${{ github.event.inputs.use_proxy }}
          CITY_INDEX=${{ github.event.inputs.city_index }}
          
          echo "Running actual scraper for single city..."
          
          CMD="python aa_hotels_scraper.py \
            --cities cities_top200.txt \
            --batch-start $CITY_INDEX \
            --batch-size 1 \
            --concurrent 1"
          
          if [ "$USE_PROXY" = "false" ]; then
            CMD="$CMD --no-proxy"
          else
            CMD="$CMD --proxies formatted_proxies.txt"
          fi
          
          echo "Command: $CMD"
          $CMD 2>&1 | tee scraper_output.log || true
          
          # Check results
          if [ -f cheapest_10k_hotels_by_city.csv ]; then
            echo "Results found:"
            cat cheapest_10k_hotels_by_city.csv
          else
            echo "No results file created"
          fi
          
          # Check checkpoint
          if [ -f checkpoint.json ]; then
            echo "Checkpoint:"
            cat checkpoint.json
          fi
          
          # Check scraper log
          if [ -f scraper.log ]; then
            echo "Scraper log tail:"
            tail -50 scraper.log
          fi

      - name: Upload debug artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            *.png
            *.log
            *.csv
            *.json
            test_city.txt
          retention-days: 7

      - name: Summary
        if: always()
        run: |
          {
            echo "## Debug Summary"
            echo ""
            echo "### Test Configuration"
            echo "- City Index: ${{ github.event.inputs.city_index }}"
            echo "- Use Proxy: ${{ github.event.inputs.use_proxy }}"
            echo "- City Tested: $(cat test_city.txt || echo 'Unknown')"
            echo ""
            echo "### Artifacts Created"
            echo "Check artifacts for:"
            echo "- Screenshots (*.png)"
            echo "- Logs (*.log)"
            echo "- Results (*.csv)"
          } >> $GITHUB_STEP_SUMMARY
