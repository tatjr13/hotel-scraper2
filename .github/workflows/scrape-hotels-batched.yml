name: Scrape Hotels Batched

on:
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of cities per batch'
        required: false
        default: '10'
        type: string

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      total_batches: ${{ steps.set-matrix.outputs.total_batches }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up batches
        id: set-matrix
        run: |
          # Read cities and create batches
          BATCH_SIZE=${{ github.event.inputs.batch_size || '10' }}

          # Count total cities
          TOTAL_CITIES=$(wc -l < cities_top200.txt)
          echo "Total cities: $TOTAL_CITIES"

          # Calculate number of batches
          TOTAL_BATCHES=$(( (TOTAL_CITIES + BATCH_SIZE - 1) / BATCH_SIZE ))
          echo "Total batches: $TOTAL_BATCHES"

          # Create matrix JSON with batch and start index
          MATRIX_JSON="{\"include\":["
          for i in $(seq 0 $((TOTAL_BATCHES - 1))); do
            if [ $i -gt 0 ]; then
              MATRIX_JSON="${MATRIX_JSON},"
            fi
            START_IDX=$((i * BATCH_SIZE))
            MATRIX_JSON="${MATRIX_JSON}{\"batch\":${i},\"start\":${START_IDX}}"
          done
          MATRIX_JSON="${MATRIX_JSON}]}"

          echo "matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          echo "total_batches=$TOTAL_BATCHES" >> $GITHUB_OUTPUT

          echo "Matrix: $MATRIX_JSON"

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      max-parallel: 5
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install pandas==2.0.3 numpy==1.24.3 playwright more_itertools aiohttp
          fi
          if pip show playwright > /dev/null 2>&1; then
            echo "Installing Playwright browsers..."
            playwright install chromium
          fi

      - name: Decrypt proxy file
        env:
          PROXY_PASSWORD: ${{ secrets.PROXY_PASSWORD }}
        run: |
          echo "Decrypting proxy file..."
          openssl enc -d -aes-256-cbc -pbkdf2 -in formatted_proxies.txt.enc -out formatted_proxies.txt -k "$PROXY_PASSWORD"
          if [ ! -f formatted_proxies.txt ]; then
            echo "Error: Proxy file decryption failed!"
            exit 1
          fi
          if [ ! -s formatted_proxies.txt ]; then
            echo "Error: Decrypted proxy file is empty!"
            exit 1
          fi
          echo "Proxy file decrypted successfully"
          echo "Proxy file size: $(wc -c < formatted_proxies.txt) bytes"

      - name: Prepare batch cities
        id: prep-batch
        run: |
          BATCH_SIZE=${{ github.event.inputs.batch_size || '10' }}
          BATCH_NUM=${{ matrix.batch }}

          START_LINE=$((BATCH_NUM * BATCH_SIZE + 1))
          END_LINE=$(((BATCH_NUM + 1) * BATCH_SIZE))

          sed -n "${START_LINE},${END_LINE}p" cities_top200.txt > cities_batch_${BATCH_NUM}.txt
          BATCH_CITY_COUNT=$(wc -l < cities_batch_${BATCH_NUM}.txt)
          echo "Batch $BATCH_NUM contains $BATCH_CITY_COUNT cities (lines $START_LINE-$END_LINE)"

          echo "batch_num=${BATCH_NUM}" >> $GITHUB_OUTPUT
          echo "batch_city_count=${BATCH_CITY_COUNT}" >> $GITHUB_OUTPUT

      - name: Run scraper for batch
        id: run-scraper
        continue-on-error: true
        run: |
          BATCH_NUM=${{ steps.prep-batch.outputs.batch_num }}

          # Scrape only this batch's cities!
          if python aa_hotels_scraper.py \
            --proxies formatted_proxies.txt \
            --cities cities_batch_${BATCH_NUM}.txt \
            --concurrent 3; then
            echo "Scraper completed successfully"
            echo "status=success" >> $GITHUB_OUTPUT

            if [ -f cheapest_10k_hotels_by_city.csv ]; then
              mv cheapest_10k_hotels_by_city.csv hotels_batch_${BATCH_NUM}.csv
              echo "Renamed output file to hotels_batch_${BATCH_NUM}.csv"
            fi
          else
            SCRAPER_EXIT=$?
            echo "Scraper failed with exit code $SCRAPER_EXIT"
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "City,Hotel,Price,Cost per Point,ProxyUsed,error_message" > hotels_batch_${BATCH_NUM}.csv
            echo "batch_${BATCH_NUM},,,,,Scraper failed" >> hotels_batch_${BATCH_NUM}.csv
          fi

          if [ -f hotels_batch_${BATCH_NUM}.csv ]; then
            echo "Output file created: $(wc -l < hotels_batch_${BATCH_NUM}.csv) lines"
            echo "Preview:"
            head -5 hotels_batch_${BATCH_NUM}.csv
          else
            echo "Warning: No output file created, creating empty one"
            echo "City,Hotel,Price,Cost per Point,ProxyUsed" > hotels_batch_${BATCH_NUM}.csv
            echo "batch_${BATCH_NUM},No data,0,0,none" >> hotels_batch_${BATCH_NUM}.csv
          fi

      - name: Upload batch results
        uses: actions/upload-artifact@v4
        with:
          name: batch-results-${{ matrix.batch }}
          path: hotels_batch_${{ matrix.batch }}.csv
          retention-days: 7

  combine-results:
    needs: [setup, scrape]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install pandas
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Download all batch results
        uses: actions/download-artifact@v4
        with:
          path: batch-results

      - name: Combine CSV files
        run: |
          cat > combine_results.py << 'PYTHON_SCRIPT'
          import os
          import pandas as pd
          import glob
          from pathlib import Path

          def combine_csv_files():
              csv_pattern = "batch-results/*/hotels_batch_*.csv"
              csv_files = glob.glob(csv_pattern)
              print(f"Found {len(csv_files)} CSV files to combine")

              if not csv_files:
                  print("No CSV files found!")
                  empty_df = pd.DataFrame(columns=['City', 'Hotel', 'Price', 'Cost per Point', 'ProxyUsed'])
                  empty_df.to_csv('hotels_all_results.csv', index=False)
                  print("Created empty results file")
                  return

              all_dfs = []
              for csv_file in sorted(csv_files):
                  print(f"Reading: {csv_file}")
                  try:
                      df = pd.read_csv(csv_file)
                      df['source_batch'] = os.path.basename(csv_file)
                      all_dfs.append(df)
                      print(f"  - Added {len(df)} rows")
                  except Exception as e:
                      print(f"  - Error reading {csv_file}: {e}")
                      continue

              if not all_dfs:
                  print("No valid data found in any CSV files!")
                  empty_df = pd.DataFrame(columns=['City', 'Hotel', 'Price', 'Cost per Point', 'ProxyUsed'])
                  empty_df.to_csv('hotels_all_results.csv', index=False)
                  return

              combined_df = pd.concat(all_dfs, ignore_index=True)

              initial_count = len(combined_df)
              if 'City' in combined_df.columns and 'Hotel' in combined_df.columns:
                  combined_df = combined_df.drop_duplicates(subset=['City', 'Hotel'], keep='first')
                  removed_count = initial_count - len(combined_df)
                  if removed_count > 0:
                      print(f"Removed {removed_count} duplicate entries")

              if 'City' in combined_df.columns:
                  combined_df = combined_df.sort_values('City')

              output_file = 'hotels_all_results.csv'
              combined_df.to_csv(output_file, index=False)
              print(f"\nCombined results saved to {output_file}")
              print(f"Total rows: {len(combined_df)}")
              print(f"Columns: {', '.join(combined_df.columns)}")
              if 'City' in combined_df.columns:
                  print(f"\nCities processed: {combined_df['City'].nunique()}")
              if 'Hotel' in combined_df.columns:
                  print(f"Total hotels: {combined_df['Hotel'].nunique()}")
              if 'Price' in combined_df.columns:
                  print(f"Average price: ${combined_df['Price'].mean():.2f}")
              print("\nFirst 5 rows of combined data:")
              print(combined_df.head().to_string())

          if __name__ == "__main__":
              combine_csv_files()
          PYTHON_SCRIPT
          python combine_results.py

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: combined-results
          path: hotels_all_results.csv
          retention-days: 30

      - name: Create summary
        run: |
          if [ -f hotels_all_results.csv ]; then
            {
              echo "## Scraping Summary"
              echo ""
              echo "- Total batches processed: ${{ needs.setup.outputs.total_batches }}"
              echo "- Combined results file: hotels_all_results.csv"
              echo "- Total rows in result: $(wc -l < hotels_all_results.csv)"
              echo ""
              echo "### Preview (first 10 lines):"
              echo '```csv'
              head -10 hotels_all_results.csv
              echo '```'
            } >> $GITHUB_STEP_SUMMARY
          else
            {
              echo "## Scraping Failed"
              echo "No combined results file was created."
            } >> $GITHUB_STEP_SUMMARY
